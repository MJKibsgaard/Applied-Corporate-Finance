{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part, we install import all the packages used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Subpackages\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "#ImportFunctions\n",
    "from Functions import fetch_data\n",
    "from Functions import moving_average\n",
    "from Functions import rsi\n",
    "from Functions import macd\n",
    "from Functions import lagged_return\n",
    "from Functions import bollinger_bands\n",
    "from Functions import pe_ratio\n",
    "from Functions import daily_returns\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import some stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [ \"WBD\", \"WM\", \"WAT\", \"WEC\", \"WFC\", \"WELL\", \"WST\", \"WDC\", \"WRK\", \"WY\", \"WHR\", \"WMB\", \"WTW\", \"GWW\", \"WYNN\", \"XEL\", \"XYL\", \"YUM\", \"ZBRA\", \"ZBH\", \"ZION\", \"ZTS\"]\n",
    "\n",
    "start_date = \"2013-10-01\"\n",
    "end_date = \"2023-10-01\"\n",
    "\n",
    "stock_data = fetch_data(tickers, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the technical indicators for each stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each stock's data, compute the technical indicators and add them as new columns to the DataFrame\n",
    "\n",
    "for ticker, data in stock_data.items():\n",
    "    # Backup the 'Close' prices\n",
    "    close_prices = data['Close'].copy()\n",
    "    \n",
    "    # 20-day Moving Average\n",
    "    data['MA20'] = moving_average(data)\n",
    "    \n",
    "    # RSI\n",
    "    data['RSI'] = rsi(data)\n",
    "    \n",
    "    # MACD\n",
    "    data['MACD'], data['Signal_Line'] = macd(data)\n",
    "    \n",
    "    # 1, 2, and 3 Day Lagged Returns\n",
    "    data['Lagged_Return_1'] = lagged_return(data, 1)\n",
    "    data['Lagged_Return_2'] = lagged_return(data, 2)\n",
    "    data['Lagged_Return_3'] = lagged_return(data, 3)\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    data['Upper_Bollinger'], data['Lower_Bollinger'] = bollinger_bands(data)\n",
    "    \n",
    "    # Restore 'Close' column\n",
    "    data['Close'] = close_prices\n",
    "\n",
    "    # Extract the 'Close' column\n",
    "    close_prices = data['Close']\n",
    "    \n",
    "    # Calculate daily returns for all available days\n",
    "    daily_returns = close_prices.pct_change().dropna()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the technical indicators for each stock we make a new dataframe that contains the data for all the stocks. We also do some data cleaning like removing the NANs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the split data for each stock\n",
    "all_stock_data = {}\n",
    "\n",
    "for ticker, data in stock_data.items():\n",
    "    data_clean = data.dropna()  # drop rows with NaN values\n",
    "    \n",
    "    # Check if the cleaned data has a minimum number of rows\n",
    "    if len(data_clean) < 10:\n",
    "        print(f\"Skipping {ticker} due to insufficient data.\")\n",
    "        continue\n",
    "\n",
    "    # Features: using technical indicators and lagged returns\n",
    "    X = data_clean[['RSI', 'MACD', 'Signal_Line', 'Lagged_Return_1', 'Lagged_Return_2', 'Lagged_Return_3', 'Upper_Bollinger', 'Lower_Bollinger']]\n",
    "\n",
    "    # Target: Direction of the next day's return (1 for positive, 0 for negative or non-positive)\n",
    "    y = (data_clean['Close'].pct_change().shift(-1) > 0).astype(int)\n",
    "\n",
    "    # Splitting the data: 70% train, 15% validation, 15% test\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=False)\n",
    "    \n",
    "    # Store the data splits in the dictionary\n",
    "    all_stock_data[ticker] = {\n",
    "        'X_train': X_train,\n",
    "        'X_val': X_val,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "\n",
    "# Now, `all_stock_data` contains the data splits for all stocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually make the classifier to predict the stock direction. We use a random forest model to determine this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Combine all stock data with a stock identifier\n",
    "all_X_train = []\n",
    "all_y_train = []\n",
    "all_X_val = []\n",
    "all_y_val = []\n",
    "\n",
    "for ticker in all_stock_data:\n",
    "    # Get data for the current stock\n",
    "    X_train_temp = all_stock_data[ticker]['X_train'].copy()\n",
    "    all_X_train.append(X_train_temp)\n",
    "    \n",
    "    y_train_temp = all_stock_data[ticker]['y_train']\n",
    "    all_y_train.append(y_train_temp)\n",
    "    \n",
    "    X_val_temp = all_stock_data[ticker]['X_val'].copy()\n",
    "    # No need to add the ticker for validation data either\n",
    "    all_X_val.append(X_val_temp)\n",
    "    \n",
    "    y_val_temp = all_stock_data[ticker]['y_val']\n",
    "    all_y_val.append(y_val_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert lists to pandas DataFrame\n",
    "all_X_train = pd.concat(all_X_train, axis=0)\n",
    "all_y_train = pd.concat(all_y_train, axis=0)\n",
    "all_X_val = pd.concat(all_X_val, axis=0)\n",
    "all_y_val = pd.concat(all_y_val, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Use RFE for feature selection\n",
    "selector = RFE(clf, n_features_to_select=3, step=1)\n",
    "selector = selector.fit(all_X_train, all_y_train)\n",
    "selected_features = selector.support_\n",
    "selected_columns = all_X_train.columns[selected_features]\n",
    "\n",
    "# Train the model with selected features\n",
    "clf.fit(all_X_train[selected_columns], all_y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_val_pred = clf.predict(all_X_val[selected_columns])\n",
    "accuracy = accuracy_score(all_y_val, y_val_pred)\n",
    "\n",
    "# Print out the performance\n",
    "print(f\"Performance for the general model:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(all_y_val, y_val_pred))\n",
    "print(\"Selected Features:\", selected_columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class RandomForestWithProgressBar(RandomForestClassifier):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        # Wrap the generator with tqdm to show progress\n",
    "        setattr(self, \"_generate_sample_indices\", \n",
    "                lambda rs, n_samples: list(tqdm(super(RandomForestClassifier, self)._generate_sample_indices(rs, n_samples), \n",
    "                                                total=self.n_estimators)))\n",
    "        return super().fit(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Random Forest Classifier. Cant get the progressbar to work...\n",
    "clf = RandomForestWithProgressBar(n_estimators=100, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = RFE(clf, n_features_to_select=3, step=1)\n",
    "selector = selector.fit(all_X_train, all_y_train)\n",
    "selected_features = selector.support_\n",
    "selected_columns = all_X_train.columns[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model with selected features\n",
    "clf.fit(all_X_train[selected_columns], all_y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_val_pred = clf.predict(all_X_val[selected_columns])\n",
    "accuracy = accuracy_score(all_y_val, y_val_pred)\n",
    "\n",
    "# Print out the performance\n",
    "print(f\"Performance for the general model:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(all_y_val, y_val_pred))\n",
    "print(\"Selected Features:\", selected_columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have trained the model on the validation set, we will now test on an out of sample test. In other words, we test on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store test set performances\n",
    "test_performances = {}\n",
    "\n",
    "# List to store all accuracy scores\n",
    "all_accuracies = []\n",
    "\n",
    "# Combine all test data with a stock identifier\n",
    "all_X_test = []\n",
    "all_y_test = []\n",
    "tickers_test_data = {}  # Store individual test data for each ticker\n",
    "\n",
    "for ticker in all_stock_data:\n",
    "    # Get the test data for the current stock\n",
    "    X_test_temp = all_stock_data[ticker]['X_test'].copy()\n",
    "    X_test_temp['ticker'] = ticker\n",
    "    all_X_test.append(X_test_temp)\n",
    "    \n",
    "    y_test_temp = all_stock_data[ticker]['y_test']\n",
    "    all_y_test.append(y_test_temp)\n",
    "    \n",
    "    # Store individual test data for each ticker (for later use)\n",
    "    tickers_test_data[ticker] = {\n",
    "        'X_test': X_test_temp,\n",
    "        'y_test': y_test_temp\n",
    "    }\n",
    "\n",
    "# Convert lists to pandas DataFrame\n",
    "all_X_test = pd.concat(all_X_test, axis=0)\n",
    "all_y_test = pd.concat(all_y_test, axis=0)\n",
    "\n",
    "# One-hot encode the 'ticker' column\n",
    "all_X_test = pd.get_dummies(all_X_test, columns=['ticker'], drop_first=True)\n",
    "\n",
    "# Use only the selected features from RFE for prediction\n",
    "all_X_test_selected = all_X_test[selected_columns]\n",
    "\n",
    "# Predict using the trained general model\n",
    "all_y_test_pred = clf.predict(all_X_test_selected)\n",
    "# Create a dictionary to store start and end positions of each ticker's data in the combined dataframe\n",
    "positions = {}\n",
    "start_pos = 0\n",
    "\n",
    "for ticker in all_stock_data:\n",
    "    end_pos = start_pos + len(tickers_test_data[ticker]['X_test'])\n",
    "    positions[ticker] = (start_pos, end_pos)\n",
    "    start_pos = end_pos\n",
    "\n",
    "# Loop through each ticker to extract and display performance\n",
    "for ticker in all_stock_data:\n",
    "    y_test_actual = tickers_test_data[ticker]['y_test']\n",
    "    \n",
    "    # Use the start and end positions to extract predictions for each ticker\n",
    "    start, end = positions[ticker]\n",
    "    y_test_pred_ticker = all_y_test_pred[start:end]\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test_actual, y_test_pred_ticker)\n",
    "    all_accuracies.append(test_accuracy)\n",
    "\n",
    "    test_performances[ticker] = {\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test Classification Report': classification_report(y_test_actual, y_test_pred_ticker)\n",
    "    }\n",
    "\n",
    "    # Print out the test performance for the current stock\n",
    "    print(f\"Test Performance for {ticker}:\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(classification_report(y_test_actual, y_test_pred_ticker))\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "# Calculate and print the average accuracy\n",
    "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
    "print(f\"Average Test Accuracy across all stocks: {average_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tested the model we want to see if we can improve the accuracy by tuning the hyperparameters. We basically run the same model as before, but this time we tune the model to find the best hyperparameters. We do this to see if we can improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set up tqdm with pandas (if you want to use it for other pandas operations)\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "# Combine all training and validation data\n",
    "all_X_train = []\n",
    "all_y_train = []\n",
    "all_X_val = []\n",
    "all_y_val = []\n",
    "\n",
    "for ticker in all_stock_data:\n",
    "    # Get data for the current stock\n",
    "    X_train_temp = all_stock_data[ticker]['X_train'].copy()\n",
    "    all_X_train.append(X_train_temp)\n",
    "    \n",
    "    y_train_temp = all_stock_data[ticker]['y_train']\n",
    "    all_y_train.append(y_train_temp)\n",
    "    \n",
    "    X_val_temp = all_stock_data[ticker]['X_val'].copy()\n",
    "    all_X_val.append(X_val_temp)\n",
    "    \n",
    "    y_val_temp = all_stock_data[ticker]['y_val']\n",
    "    all_y_val.append(y_val_temp)\n",
    "\n",
    "# Convert lists to pandas DataFrame\n",
    "all_X_train = pd.concat(all_X_train, axis=0)\n",
    "all_y_train = pd.concat(all_y_train, axis=0)\n",
    "all_X_val = pd.concat(all_X_val, axis=0)\n",
    "all_y_val = pd.concat(all_y_val, axis=0)\n",
    "\n",
    "# Define the Random Forest with progress bar\n",
    "class RandomForestWithProgressBar(RandomForestClassifier):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        # Wrap the generator with tqdm to show progress\n",
    "        setattr(self, \"_generate_sample_indices\", \n",
    "                lambda rs, n_samples: list(tqdm(super(RandomForestClassifier, self)._generate_sample_indices(rs, n_samples), \n",
    "                                                total=self.n_estimators, desc=\"Random Forest Progress\")))\n",
    "        return super().fit(*args, **kwargs)\n",
    "\n",
    "# Create a pipeline with RFE and RandomForestClassifier\n",
    "pipe = Pipeline([\n",
    "    ('rfe', RFE(estimator=RandomForestWithProgressBar(random_state=42))),\n",
    "    ('clf', RandomForestWithProgressBar(random_state=42))\n",
    "])\n",
    "\n",
    "# Distributions for hyperparameters\n",
    "param_dist = {\n",
    "    'rfe__n_features_to_select': [2, 4, 6, 8], \n",
    "    'clf__n_estimators': [50, 100, 150, 200],\n",
    "    'clf__max_depth': [None, 10, 20, 30, 40],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Randomized Search CV\n",
    "random_search = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=100, cv=5, scoring='accuracy', random_state=42, n_jobs=-1, verbose=10)\n",
    "random_search.fit(all_X_train, all_y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_val_pred = random_search.predict(all_X_val)\n",
    "accuracy = accuracy_score(all_y_val, y_val_pred)\n",
    "\n",
    "print(f\"Tuned Performance for the general model:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(all_y_val, y_val_pred))\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"-------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tuned the model on the training and validation data, we want to see how well it performs on an out of sample dataset. We use the tuned models on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store test set performances for the tuned models\n",
    "tuned_test_performances = {}\n",
    "\n",
    "# List to store all test accuracy scores for the tuned models\n",
    "all_tuned_test_accuracies = []\n",
    "\n",
    "# Combine all test data\n",
    "all_X_test = pd.concat([all_stock_data[ticker]['X_test'] for ticker in all_stock_data], axis=0)\n",
    "all_y_test = pd.concat([all_stock_data[ticker]['y_test'] for ticker in all_stock_data], axis=0)\n",
    "\n",
    "# Predict using the tuned general model\n",
    "all_y_test_pred = random_search.predict(all_X_test)\n",
    "\n",
    "# Create a mask for each ticker and calculate performance\n",
    "start_idx = 0\n",
    "for ticker in all_stock_data:\n",
    "    end_idx = start_idx + len(all_stock_data[ticker]['y_test'])\n",
    "    y_test_actual = all_y_test[start_idx:end_idx]\n",
    "    y_test_pred_ticker = all_y_test_pred[start_idx:end_idx]\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test_actual, y_test_pred_ticker)\n",
    "    all_tuned_test_accuracies.append(test_accuracy)\n",
    "\n",
    "    tuned_test_performances[ticker] = {\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test Classification Report': classification_report(y_test_actual, y_test_pred_ticker, zero_division=1)\n",
    "    }\n",
    "\n",
    "    print(f\"Tuned Test Performance for {ticker}:\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(classification_report(y_test_actual, y_test_pred_ticker, zero_division=1))\n",
    "    print(\"-------------------------------------------------\")\n",
    "    \n",
    "    start_idx = end_idx\n",
    "\n",
    "average_tuned_test_accuracy = sum(all_tuned_test_accuracies) / len(all_tuned_test_accuracies)\n",
    "print(f\"\\nAverage Tuned Test Accuracy across all stocks: {average_tuned_test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done the gridsearch we can evaluate the two models based on their accuracy. We make a table of the results from the validation test and the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Tuned Model Results\n",
    "non_tuned_validation_accuracy = accuracy  # From the non-tuned model validation\n",
    "non_tuned_test_accuracy = average_accuracy  # From the non-tuned model test\n",
    "\n",
    "# Tuned Model Results\n",
    "tuned_validation_accuracy = accuracy  # From the tuned model validation\n",
    "tuned_test_accuracy = average_tuned_test_accuracy  # From the tuned model test\n",
    "\n",
    "# Create a DataFrame to compile the results\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Non-Tuned', 'Tuned'],\n",
    "    'Validation Accuracy': [non_tuned_validation_accuracy, tuned_validation_accuracy],\n",
    "    'Test Accuracy': [non_tuned_test_accuracy, tuned_test_accuracy]\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start backtesting the models. We make a list that includes the predicted return and the actual return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list to store results\n",
    "stock_results = []\n",
    "\n",
    "# Keep track of the current index in all_X_test and all_y_test\n",
    "current_idx = 0\n",
    "\n",
    "# Loop through each ticker to extract predictions and actual returns\n",
    "for ticker in all_stock_data:\n",
    "    # Calculate the number of test samples for the current ticker\n",
    "    num_samples = len(all_stock_data[ticker]['X_test'])\n",
    "    \n",
    "    # Slice the test data for the current ticker\n",
    "    X_test_ticker = all_X_test.iloc[current_idx:current_idx + num_samples]\n",
    "    y_test_actual = all_y_test.iloc[current_idx:current_idx + num_samples]\n",
    "    \n",
    "    # Predict using the non-tuned model (clf from previous code)\n",
    "    non_tuned_predictions = clf.predict(X_test_ticker[selected_columns])\n",
    "    \n",
    "    # Predict using the tuned model (random_search from previous code)\n",
    "    tuned_predictions = random_search.predict(X_test_ticker)\n",
    "    \n",
    "    # Calculate the actual daily return for the stock\n",
    "    # Access the 'Close' column directly from the original stock_data\n",
    "    daily_returns = stock_data[ticker].loc[all_stock_data[ticker]['X_test'].index]['Close'].pct_change().shift(-1).dropna()\n",
    "    \n",
    "    # Append the results to the stock_results list\n",
    "    stock_results.append({\n",
    "        'Ticker': ticker,\n",
    "        'Non-Tuned Predictions': non_tuned_predictions,\n",
    "        'Tuned Predictions': tuned_predictions,\n",
    "        'Actual Daily Returns': daily_returns.values\n",
    "    })\n",
    "\n",
    "    # Update the current index for the next iteration\n",
    "    current_idx += num_samples\n",
    "\n",
    "# Now, the stock_results list contains the predictions from both models and the actual daily returns for each stock.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the backtest and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store cumulative returns for each strategy\n",
    "cumulative_returns_tuned = []\n",
    "cumulative_returns_nontuned = []\n",
    "cumulative_returns_hold = []\n",
    "\n",
    "for result in stock_results:\n",
    "    # Extract only the required number of predictions equal to the length of actual daily returns\n",
    "    actual_daily_returns = result['Actual Daily Returns']\n",
    "    tuned_strategy_returns = (result['Tuned Predictions'][:len(actual_daily_returns)] * 2) - 1\n",
    "    nontuned_strategy_returns = (result['Non-Tuned Predictions'][:len(actual_daily_returns)] * 2) - 1\n",
    "    \n",
    "    # Calculate daily strategy returns\n",
    "    tuned_daily_returns = tuned_strategy_returns * actual_daily_returns\n",
    "    nontuned_daily_returns = nontuned_strategy_returns * actual_daily_returns\n",
    "    \n",
    "    # Calculate the cumulative returns\n",
    "    cumulative_returns_tuned.append((1 + tuned_daily_returns).cumprod() - 1)\n",
    "    cumulative_returns_nontuned.append((1 + nontuned_daily_returns).cumprod() - 1)\n",
    "    cumulative_returns_hold.append((1 + actual_daily_returns).cumprod() - 1)\n",
    "\n",
    "# Calculate the mean cumulative returns across all stocks for each day\n",
    "mean_cumulative_returns_tuned = np.nanmean(np.array([np.pad(cr, (0, max(map(len, cumulative_returns_tuned)) - len(cr)), 'edge') for cr in cumulative_returns_tuned]), axis=0)\n",
    "mean_cumulative_returns_nontuned = np.nanmean(np.array([np.pad(cr, (0, max(map(len, cumulative_returns_nontuned)) - len(cr)), 'edge') for cr in cumulative_returns_nontuned]), axis=0)\n",
    "mean_cumulative_returns_hold = np.nanmean(np.array([np.pad(cr, (0, max(map(len, cumulative_returns_hold)) - len(cr)), 'edge') for cr in cumulative_returns_hold]), axis=0)\n",
    "\n",
    "# Plot the cumulative returns\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean_cumulative_returns_tuned, label=\"Tuned Model Strategy\", color='blue')\n",
    "plt.plot(mean_cumulative_returns_nontuned, label=\"Non-Tuned Model Strategy\", color='green')\n",
    "plt.plot(mean_cumulative_returns_hold, label=\"Hold Strategy\", color='red')\n",
    "plt.title(\"Average Cumulative Returns of Different Strategies\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Average Cumulative Returns\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the sharpe ratios from the 3 strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the daily returns for each strategy\n",
    "daily_returns_tuned = np.diff(np.array(mean_cumulative_returns_tuned) + 1)\n",
    "daily_returns_nontuned = np.diff(np.array(mean_cumulative_returns_nontuned) + 1)\n",
    "daily_returns_hold = np.diff(np.array(mean_cumulative_returns_hold) + 1)\n",
    "\n",
    "# Calculate Sharpe Ratios\n",
    "sharpe_ratio_tuned = daily_returns_tuned.mean() / daily_returns_tuned.std()\n",
    "sharpe_ratio_nontuned = daily_returns_nontuned.mean() / daily_returns_nontuned.std()\n",
    "sharpe_ratio_hold = daily_returns_hold.mean() / daily_returns_hold.std()\n",
    "\n",
    "# Print the Sharpe Ratios\n",
    "print(\"Sharpe Ratio for Tuned Model Strategy:\", sharpe_ratio_tuned)\n",
    "print(\"Sharpe Ratio for Non-Tuned Model Strategy:\", sharpe_ratio_nontuned)\n",
    "print(\"Sharpe Ratio for Hold Strategy:\", sharpe_ratio_hold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next part of the code will focus more on evaluating the machine learning models. Here we check what kind of parameters the model uses and how well it guesses the direction of the stocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the trained RandomForest model from the RandomizedSearchCV best estimator\n",
    "rf_model = random_search.best_estimator_.named_steps['clf']\n",
    "\n",
    "# Get feature importances from the model\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# Match the importances with the feature names\n",
    "features = all_X_train.columns\n",
    "feature_importance_dict = dict(zip(features, feature_importances))\n",
    "\n",
    "# Sort the dictionary by importance\n",
    "sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the sorted feature importances\n",
    "for feature, importance in sorted_feature_importance:\n",
    "    print(f\"Feature: {feature}, Importance: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract feature names and their importances\n",
    "features, importances = zip(*sorted_feature_importance)\n",
    "\n",
    "# Create the vertical bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(features, importances, align='center')\n",
    "plt.xticks(rotation=45)  # Rotate feature names for better readability\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Feature')\n",
    "plt.title('Feature Importances for the Tuned Model')\n",
    "plt.tight_layout()  # Adjusts subplot parameters for better layout\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above we have the most important features of the tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test how many times the model actually gets the correct answer and how many times it is wrong. We plot a confusion matrix for the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predictions from the models\n",
    "y_pred_tuned = random_search.predict(all_X_val)\n",
    "y_pred_nontuned = clf.predict(all_X_val[selected_columns])  # Using the non-tuned model and selected columns from earlier\n",
    "\n",
    "# Confusion matrices\n",
    "cm_tuned = confusion_matrix(all_y_val, y_pred_tuned)\n",
    "cm_nontuned = confusion_matrix(all_y_val, y_pred_nontuned)\n",
    "\n",
    "# Plotting the confusion matrix for the tuned model\n",
    "plt.figure(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay(cm_tuned, display_labels=['Down', 'Up']).plot(cmap=plt.cm.Blues, values_format='.0f')\n",
    "plt.title('Confusion Matrix for the Tuned Model')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the confusion matrix for the non-tuned model\n",
    "plt.figure(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay(cm_nontuned, display_labels=['Down', 'Up']).plot(cmap=plt.cm.Blues, values_format='.0f')\n",
    "plt.title('Confusion Matrix for the Non-Tuned Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a heatmap for the effects of the hyperparameters on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Extract results from RandomizedSearchCV\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "# Pivot the results for heatmap\n",
    "heatmap_data = results.pivot_table(values='mean_test_score', \n",
    "                                   index='param_clf__n_estimators', \n",
    "                                   columns='param_clf__max_depth')\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".3f\", cmap='plasma', cbar_kws={'label': 'Mean Test Score'})\n",
    "plt.title('Hyperparameter Influence on Accuracy')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Number of Estimators')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROCAUC comes downunder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the predicted probabilities for the positive class (stock going up)\n",
    "y_pred_prob_tuned = random_search.predict_proba(all_X_val)[:, 1]\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(all_y_val, y_pred_prob_tuned)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Extract AUC scores and iterations from the RandomizedSearchCV results\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "iterations = results.index\n",
    "mean_test_scores = results[\"mean_test_score\"]\n",
    "\n",
    "# 2. Fit a polynomial regression to the data\n",
    "degree = 2\n",
    "p = Polynomial.fit(iterations, mean_test_scores, degree)\n",
    "x_new = np.linspace(iterations.min(), iterations.max(), 500)\n",
    "y_new = p(x_new)\n",
    "\n",
    "# 3. Compute the standard deviation of the mean test scores to use for error bands\n",
    "std_dev = mean_test_scores.std()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(iterations, mean_test_scores, 'o', label='AUC vs. Iterations', markersize=5)\n",
    "plt.plot(x_new, y_new, '-', label=f'Polynomial Regression (degree={degree})')\n",
    "plt.fill_between(x_new, y_new - std_dev, y_new + std_dev, color='grey', alpha=0.2, label='Error Bands (±1 std. dev.)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('AUC vs. Iterations with Polynomial Regression & Error Bands')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ovenstående plot viser hvor meget præcisionen øges når man øger antallet af iterationer i vores random gridsearch. Det hjælper ikke meget mere for præcisionen, så det giver ikke meget mening at øge det. Det ligner at den konvergerer mod det der punkt. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
